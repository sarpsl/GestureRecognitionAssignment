{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwYBFSxzfSm0"
      },
      "source": [
        "# Gesture Recognition\n",
        "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <font color='blue'>Index</font>\n",
        "- <font color='blue'>1. Initial Steps</font>\n",
        "\t- <font color='blue'>1.1 Imports</font>\n",
        "\t- <font color='blue'>1.2 Random Seeds</font>\n",
        "\t- <font color='blue'>1.3 Google Drive Connect</font>\n",
        "\t- <font color='blue'>1.4 Setting Paths</font>\n",
        "\t- <font color='blue'>1.5 Defining Generator</font>\n",
        "\t- <font color='blue'>1.6 Listing File Counts</font>\n",
        "- <font color='blue'>2. Models</font>\n",
        "\t- <font color='blue'>2.1 Model 1 - </font>\n",
        "\t\t- <font color='blue'>2.1.1 Design, Compile & Summary</font>\n",
        "\t\t- <font color='blue'>2.1.2 Model Training</font>\n",
        "\t\t- <font color='blue'>2.1.3 Accuracy & Remarks</font>\n",
        "\t\t- <font color='blue'>2.1.4 Next Actions</font>\n",
        "\t- <font color='blue'>2.2 Model 2 - </font>\n",
        "\t\t- <font color='blue'>2.2.1 Design, Compile & Summary</font>\n",
        "\t\t- <font color='blue'>2.2.2 Model Training</font>\n",
        "\t\t- <font color='blue'>2.2.3 Accuracy & Remarks</font>\n",
        "\t\t- <font color='blue'>2.2.4 Next Actions</font>\n",
        "\t- <font color='blue'>2.3 Model 3 - </font>\n",
        "\t\t- <font color='blue'>2.3.1 Design, Compile & Summary</font>\n",
        "\t\t- <font color='blue'>2.3.2 Model Training</font>\n",
        "\t\t- <font color='blue'>2.3.3 Accuracy & Remarks</font>\n",
        "\t\t- <font color='blue'>2.3.4 Next Actions</font>\n",
        "\t- <font color='blue'>2.4 Model 4 - </font>\n",
        "\t\t- <font color='blue'>2.4.1 Design, Compile & Summary</font>\n",
        "\t\t- <font color='blue'>2.4.2 Model Training</font>\n",
        "\t\t- <font color='blue'>2.4.3 Accuracy & Remarks</font>\n",
        "\t\t- <font color='blue'>2.4.4 Next Actions</font>\n",
        "\t- <font color='blue'>2.5 Model 5 - </font>\n",
        "\t\t- <font color='blue'>2.5.1 Design, Compile & Summary</font>\n",
        "\t\t- <font color='blue'>2.5.2 Model Training</font>\n",
        "\t\t- <font color='blue'>2.5.3 Accuracy & Remarks</font>\n",
        "\t\t- <font color='blue'>2.5.4 Next Actions</font>\n",
        "\t- <font color='blue'>2.6 Model 6 - </font>\n",
        "\t\t- <font color='blue'>2.6.1 Design, Compile & Summary</font>\n",
        "\t\t- <font color='blue'>2.6.2 Model Training</font>\n",
        "\t\t- <font color='blue'>2.6.3 Accuracy & Remarks</font>\n",
        "\t\t- <font color='blue'>2.6.4 Next Actions</font>\n",
        "- <font color='blue'>3. Conclusion & Final Selection</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <font color='blue'>1. Initial Steps</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font color='blue'>1.1 Imports</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "yAotwUfcfSm1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import imageio.v2 as imageio\n",
        "from imageio.v2 import imread\n",
        "from skimage.transform import resize\n",
        "import datetime\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout\n",
        "from keras.layers import Conv3D, MaxPooling3D\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras import optimizers\n",
        "\n",
        "from keras.applications import mobilenet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font color='blue'>1.2 Random Seeds</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D24Jv6apfSm3"
      },
      "source": [
        "We set the random seed so that the results don't vary drastically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "aiewGHh_fSm3"
      },
      "outputs": [],
      "source": [
        "np.random.seed(30)\n",
        "import random as rn\n",
        "rn.seed(30)\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGLD34vMfSm3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font color='blue'>1.3 Google Drive Connect</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkaxqxbDfSm3",
        "outputId": "0ec2cb13-97be-49ee-ce4f-dd6caea1beb1"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font color='blue'>1.4 Setting Paths</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "HG7Zs-L-g7LF"
      },
      "outputs": [],
      "source": [
        "train_doc = np.random.permutation(open('/content/gdrive/My Drive/Personal/upGrad/MS/Recurrent Neural Networks/Project_data/train.csv').readlines())\n",
        "val_doc = np.random.permutation(open('/content/gdrive/My Drive/Personal/upGrad/MS/Recurrent Neural Networks/Project_data/val.csv').readlines())\n",
        "\n",
        "# train_doc = np.random.permutation(open('G:\\\\My Drive\\\\Personal\\\\upGrad\\\\MS\\\\Recurrent Neural Networks\\\\Project_data\\\\train.csv').readlines())\n",
        "# val_doc = np.random.permutation(open('G:\\\\My Drive\\\\Personal\\\\upGrad\\\\MS\\\\Recurrent Neural Networks\\\\Project_data\\\\val.csv').readlines())\n",
        "\n",
        "batch_size = 64 #experiment with the batch size 16, 32, 64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font color='blue'>1.5 Defining Generator</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJtMUnfmfSm4"
      },
      "source": [
        "## Generator\n",
        "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "36MY7krNfSm4"
      },
      "outputs": [],
      "source": [
        "def generator(source_path, folder_list, batch_size, img_idx, image_height, image_width):\n",
        "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
        "    # img_idx = [1,4,7,10,13,16,19,22,25,28] #create a list of image numbers you want to use for a particular video\n",
        "    while True:\n",
        "        t = np.random.permutation(folder_list)\n",
        "        num_batches = int(len(t)/batch_size) # calculate the number of batches\n",
        "        # left over batches which should be handled separately\n",
        "        leftover_batches = len(t) - num_batches * batch_size\n",
        "\n",
        "        for batch in range(num_batches): # we iterate over the number of batches\n",
        "            batch_data = np.zeros((batch_size, len(img_idx), image_height, image_width, 3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
        "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
        "            for folder in range(batch_size): # iterate over the batch_size\n",
        "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
        "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
        "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
        "\n",
        "                    #crop the images and resize them. Note that the images are of 2 different shape\n",
        "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
        "                    image = resize(image, (image_height, image_width))\n",
        "                    batch_data[folder,idx,:,:,0] = (image[:,:,0]) - 104\n",
        "                    batch_data[folder,idx,:,:,1] = (image[:,:,1]) - 117\n",
        "                    batch_data[folder,idx,:,:,2] = (image[:,:,2]) - 123\n",
        "\n",
        "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
        "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
        "\n",
        "\n",
        "        # write the code for the remaining data points which are left after full batches\n",
        "        # write the code for the remaining data points which are left after full batches\n",
        "        if leftover_batches != 0:\n",
        "            for batch in range(num_batches):\n",
        "                # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
        "                batch_data = np.zeros((batch_size,len(img_idx),image_height, image_width,3))\n",
        "                # batch_labels is the one hot representation of the output: 10 videos with 5 columns as classes\n",
        "                batch_labels = np.zeros((batch_size,5))\n",
        "                for folder in range(batch_size): # iterate over the batch_size\n",
        "                    imgs = os.listdir(source_path +'/'+t[batch * batch_size + folder].split(';')[0])\n",
        "                    for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
        "\n",
        "                        image = imageio.imread(source_path +'/'+t[batch * batch_size + folder].split(';')[0] +'/'+imgs[item]).astype(np.float32)\n",
        "                        image = resize(image, (image_height,image_width))\n",
        "\n",
        "                        batch_data[folder,idx,:,:,0] = (image[:,:,0]) - 104\n",
        "                        batch_data[folder,idx,:,:,1] = (image[:,:,1]) - 117\n",
        "                        batch_data[folder,idx,:,:,2] = (image[:,:,2]) - 123\n",
        "\n",
        "                    #Fill the one hot encoding stuff where we maintain the label\n",
        "                    batch_labels[folder, int(t[batch * batch_size + folder].split(';')[2])] = 1\n",
        "                yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUEXzkB6fSm4"
      },
      "source": [
        "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font color='blue'>1.6 Listing File Counts</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsV9JJxIfSm4",
        "outputId": "56372afe-17a5-4d91-c759-b4a3f9936396"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# training sequences = 663\n",
            "# validation sequences = 100\n",
            "# epochs = 15\n"
          ]
        }
      ],
      "source": [
        "curr_dt_time = datetime.datetime.now()\n",
        "train_path = '/content/gdrive/My Drive/Personal/upGrad/MS/Recurrent Neural Networks/Project_data/train'\n",
        "val_path = '/content/gdrive/My Drive/Personal/upGrad/MS/Recurrent Neural Networks/Project_data/val'\n",
        "model_path_prefix = '/content/gdrive/My Drive/Personal/upGrad/MS/Recurrent Neural Networks/Models/'\n",
        "# train_path = 'G:\\\\My Drive\\\\Personal\\\\upGrad\\\\MS\\\\Recurrent Neural Networks\\\\Project_data\\\\train'\n",
        "# val_path = 'G:\\\\My Drive\\\\Personal\\\\upGrad\\\\MS\\\\Recurrent Neural Networks\\\\Project_data\\\\val'\n",
        "# model_path_prefix = 'C:\\\\Temp\\\\RNN\\\\'\n",
        "num_train_sequences = len(train_doc)\n",
        "print('# training sequences =', num_train_sequences)\n",
        "num_val_sequences = len(val_doc)\n",
        "print('# validation sequences =', num_val_sequences)\n",
        "num_epochs = 15 # choose the number of epochs\n",
        "print ('# epochs =', num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i26PWoagGCPz",
        "outputId": "b6458f6e-2bc1-4f7f-9049-2cb211739d78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Source path =  G:\\My Drive\\Personal\\upGrad\\MS\\Recurrent Neural Networks\\Project_data\\train ; batch size = 1\n"
          ]
        }
      ],
      "source": [
        "img_idx = list(range(1,30,3))\n",
        "image_height = 50\n",
        "image_width = 50\n",
        "batch_size = 1\n",
        "test_gen = generator(train_path, train_doc, batch_size, img_idx, image_height, image_width)\n",
        "d = next(test_gen)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "qxU9V0cROUeS",
        "outputId": "8c254130-0fc8-4f4f-9b50-422b973391a4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x25a41754950>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=2)\n",
        "plt.imshow(d[0,9,:,:,:])\n",
        "# axes[1].imshow(d[3,15,:,:,:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <font color='blue'>2. Models</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font color='blue'>2.1 Model 1 - </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='blue'>2.1.1 Design, Compile & Summary</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbYgj_BpfSm5",
        "outputId": "7e357750-e20e-43a3-b699-e9d7ae30425c",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\ASyed\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\ASyed\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv3d (Conv3D)             (None, 10, 50, 50, 16)    1312      \n",
            "                                                                 \n",
            " max_pooling3d (MaxPooling3  (None, 5, 25, 25, 16)     0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 5, 25, 25, 16)     64        \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " conv3d_1 (Conv3D)           (None, 5, 25, 25, 32)     13856     \n",
            "                                                                 \n",
            " max_pooling3d_1 (MaxPoolin  (None, 2, 12, 12, 32)     0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 2, 12, 12, 32)     128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv3d_2 (Conv3D)           (None, 2, 12, 12, 64)     55360     \n",
            "                                                                 \n",
            " max_pooling3d_2 (MaxPoolin  (None, 1, 6, 6, 64)       0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 1, 6, 6, 64)       256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2304)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               295040    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 5)                 645       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 366661 (1.40 MB)\n",
            "Trainable params: 366437 (1.40 MB)\n",
            "Non-trainable params: 224 (896.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "img_idx = list(range(1,30,3))\n",
        "frames = len(img_idx)\n",
        "image_height = 50\n",
        "image_width = 50\n",
        "batch_size = 1\n",
        "num_classes = 5\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv3D(16, (3,3,3), padding='same', input_shape=(frames, image_height, image_width, 3), activation='relu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv3D(32, (3,3,3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv3D(64, (3,3,3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "optimiser = \"adam\"\n",
        "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "print (model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='blue'>2.1.2 Model Training</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ut5YMGEfSm6"
      },
      "source": [
        "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAukElrPfSm6",
        "outputId": "bde51969-2835-49ec-85f7-94d5ab8c6502"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Source path =  G:\\My Drive\\Personal\\upGrad\\MS\\Recurrent Neural Networks\\Project_data\\train ; batch size = 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ASyed\\AppData\\Local\\Temp\\ipykernel_46732\\4257311695.py:26: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\ASyed\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\ASyed\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
            "\n",
            "663/663 [==============================] - ETA: 0s - loss: 1.8976 - categorical_accuracy: 0.3952Source path =  G:\\My Drive\\Personal\\upGrad\\MS\\Recurrent Neural Networks\\Project_data\\val ; batch size = 1\n",
            "\n",
            "Epoch 1: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00001-1.89764-0.39517-2.16167-0.49000.h5\n",
            "663/663 [==============================] - 254s 376ms/step - loss: 1.8976 - categorical_accuracy: 0.3952 - val_loss: 2.1617 - val_categorical_accuracy: 0.4900 - lr: 0.0010\n",
            "Epoch 2/15\n",
            "  1/663 [..............................] - ETA: 12s - loss: 1.2494 - categorical_accuracy: 0.0000e+00"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ASyed\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "663/663 [==============================] - ETA: 0s - loss: 1.0559 - categorical_accuracy: 0.5958\n",
            "Epoch 2: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00002-1.05591-0.59578-1.08228-0.67000.h5\n",
            "663/663 [==============================] - 197s 297ms/step - loss: 1.0559 - categorical_accuracy: 0.5958 - val_loss: 1.0823 - val_categorical_accuracy: 0.6700 - lr: 0.0010\n",
            "Epoch 3/15\n",
            "663/663 [==============================] - ETA: 0s - loss: 0.6762 - categorical_accuracy: 0.7602\n",
            "Epoch 3: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00003-0.67621-0.76018-0.92267-0.74000.h5\n",
            "663/663 [==============================] - 193s 292ms/step - loss: 0.6762 - categorical_accuracy: 0.7602 - val_loss: 0.9227 - val_categorical_accuracy: 0.7400 - lr: 0.0010\n",
            "Epoch 4/15\n",
            "663/663 [==============================] - ETA: 0s - loss: 0.4351 - categorical_accuracy: 0.8296\n",
            "Epoch 4: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00004-0.43515-0.82956-1.05031-0.67000.h5\n",
            "663/663 [==============================] - 186s 281ms/step - loss: 0.4351 - categorical_accuracy: 0.8296 - val_loss: 1.0503 - val_categorical_accuracy: 0.6700 - lr: 0.0010\n",
            "Epoch 5/15\n",
            "663/663 [==============================] - ETA: 0s - loss: 0.3042 - categorical_accuracy: 0.8793\n",
            "Epoch 5: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00005-0.30422-0.87934-1.45448-0.68000.h5\n",
            "663/663 [==============================] - 179s 271ms/step - loss: 0.3042 - categorical_accuracy: 0.8793 - val_loss: 1.4545 - val_categorical_accuracy: 0.6800 - lr: 0.0010\n",
            "Epoch 6/15\n",
            "663/663 [==============================] - ETA: 0s - loss: 0.2972 - categorical_accuracy: 0.8914\n",
            "Epoch 6: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00006-0.29718-0.89140-1.77237-0.65000.h5\n",
            "663/663 [==============================] - 177s 268ms/step - loss: 0.2972 - categorical_accuracy: 0.8914 - val_loss: 1.7724 - val_categorical_accuracy: 0.6500 - lr: 0.0010\n",
            "Epoch 7/15\n",
            "663/663 [==============================] - ETA: 0s - loss: 0.2699 - categorical_accuracy: 0.9186\n",
            "Epoch 7: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00007-0.26995-0.91855-0.92186-0.79000.h5\n",
            "663/663 [==============================] - 173s 261ms/step - loss: 0.2699 - categorical_accuracy: 0.9186 - val_loss: 0.9219 - val_categorical_accuracy: 0.7900 - lr: 0.0010\n",
            "Epoch 8/15\n",
            "663/663 [==============================] - ETA: 0s - loss: 0.1083 - categorical_accuracy: 0.9623\n",
            "Epoch 8: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00008-0.10829-0.96229-1.34807-0.76000.h5\n",
            "663/663 [==============================] - 161s 243ms/step - loss: 0.1083 - categorical_accuracy: 0.9623 - val_loss: 1.3481 - val_categorical_accuracy: 0.7600 - lr: 0.0010\n",
            "Epoch 9/15\n",
            "663/663 [==============================] - ETA: 0s - loss: 0.1660 - categorical_accuracy: 0.9563\n",
            "Epoch 9: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00009-0.16597-0.95626-2.78983-0.65000.h5\n",
            "663/663 [==============================] - 195s 295ms/step - loss: 0.1660 - categorical_accuracy: 0.9563 - val_loss: 2.7898 - val_categorical_accuracy: 0.6500 - lr: 0.0010\n",
            "Epoch 10/15\n",
            "663/663 [==============================] - ETA: 0s - loss: 0.2557 - categorical_accuracy: 0.9065\n",
            "Epoch 10: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00010-0.25571-0.90649-0.91785-0.82000.h5\n",
            "663/663 [==============================] - 186s 281ms/step - loss: 0.2557 - categorical_accuracy: 0.9065 - val_loss: 0.9179 - val_categorical_accuracy: 0.8200 - lr: 0.0010\n",
            "Epoch 11/15\n",
            "663/663 [==============================] - ETA: 0s - loss: 0.0689 - categorical_accuracy: 0.9759\n",
            "Epoch 11: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00011-0.06892-0.97587-1.79006-0.75000.h5\n",
            "663/663 [==============================] - 168s 254ms/step - loss: 0.0689 - categorical_accuracy: 0.9759 - val_loss: 1.7901 - val_categorical_accuracy: 0.7500 - lr: 0.0010\n",
            "Epoch 12/15\n",
            "663/663 [==============================] - ETA: 0s - loss: 0.1075 - categorical_accuracy: 0.9578\n",
            "Epoch 12: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00012-0.10747-0.95777-1.31524-0.81000.h5\n",
            "663/663 [==============================] - 153s 232ms/step - loss: 0.1075 - categorical_accuracy: 0.9578 - val_loss: 1.3152 - val_categorical_accuracy: 0.8100 - lr: 0.0010\n",
            "Epoch 13/15\n",
            "663/663 [==============================] - ETA: 0s - loss: 0.0290 - categorical_accuracy: 0.9894\n",
            "Epoch 13: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00013-0.02900-0.98944-1.34345-0.82000.h5\n",
            "663/663 [==============================] - 160s 241ms/step - loss: 0.0290 - categorical_accuracy: 0.9894 - val_loss: 1.3434 - val_categorical_accuracy: 0.8200 - lr: 0.0010\n",
            "Epoch 14/15\n",
            "663/663 [==============================] - ETA: 0s - loss: 0.0119 - categorical_accuracy: 0.9970\n",
            "Epoch 14: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00014-0.01188-0.99698-1.49615-0.83000.h5\n",
            "663/663 [==============================] - 183s 276ms/step - loss: 0.0119 - categorical_accuracy: 0.9970 - val_loss: 1.4962 - val_categorical_accuracy: 0.8300 - lr: 0.0010\n",
            "Epoch 15/15\n",
            "663/663 [==============================] - ETA: 0s - loss: 0.0020 - categorical_accuracy: 1.0000\n",
            "Epoch 15: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00015-0.00201-1.00000-1.21714-0.86000.h5\n",
            "663/663 [==============================] - 198s 299ms/step - loss: 0.0020 - categorical_accuracy: 1.0000 - val_loss: 1.2171 - val_categorical_accuracy: 0.8600 - lr: 0.0010\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x25a419f1e90>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_generator = generator(train_path, train_doc, batch_size, img_idx, image_height, image_width)\n",
        "val_generator = generator(val_path, val_doc, batch_size, img_idx, image_height, image_width)\n",
        "\n",
        "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
        "\n",
        "if not os.path.exists(model_name):\n",
        "    os.mkdir(model_name)\n",
        "\n",
        "filepath = model_path_prefix + model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
        "\n",
        "LR = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=5, min_lr=0.001) # write the REducelronplateau code here\n",
        "callbacks_list = [checkpoint, LR]\n",
        "\n",
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1\n",
        "    \n",
        "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
        "                    callbacks=callbacks_list, validation_data=val_generator,\n",
        "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='blue'>2.1.3 Accuracy & Remarks</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<b>Accuracy</b>:\n",
        "- Training: 1.0\n",
        "- Validation: 0.86\n",
        "\n",
        "<b>Remarks:</b>\n",
        "- Started with batch size 1 to check the model is working \n",
        "- Validation accuracy is not bad\n",
        "- High difference between Training vs Validation, shows model to be overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='blue'>2.1.4 Next Actions</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Increase Batch Size\n",
        "- Increase Image Height and Width\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font color='blue'>2.2 Model 2 - </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='blue'>2.2.1 Design, Compile & Summary</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv3d_6 (Conv3D)           (None, 10, 100, 100, 16   1312      \n",
            "                             )                                   \n",
            "                                                                 \n",
            " max_pooling3d_6 (MaxPoolin  (None, 5, 50, 50, 16)     0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " batch_normalization_6 (Bat  (None, 5, 50, 50, 16)     64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv3d_7 (Conv3D)           (None, 5, 50, 50, 32)     13856     \n",
            "                                                                 \n",
            " max_pooling3d_7 (MaxPoolin  (None, 2, 25, 25, 32)     0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " batch_normalization_7 (Bat  (None, 2, 25, 25, 32)     128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv3d_8 (Conv3D)           (None, 2, 25, 25, 64)     55360     \n",
            "                                                                 \n",
            " max_pooling3d_8 (MaxPoolin  (None, 1, 12, 12, 64)     0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " batch_normalization_8 (Bat  (None, 1, 12, 12, 64)     256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 9216)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 128)               1179776   \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 5)                 645       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1251397 (4.77 MB)\n",
            "Trainable params: 1251173 (4.77 MB)\n",
            "Non-trainable params: 224 (896.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "img_idx = list(range(1,30,3))\n",
        "frames = len(img_idx)\n",
        "image_height = 100\n",
        "image_width = 100\n",
        "batch_size = 32\n",
        "num_classes = 5\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv3D(16, (3,3,3), padding='same', input_shape=(frames, image_height, image_width, 3), activation='relu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv3D(32, (3,3,3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv3D(64, (3,3,3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "optimiser = \"adam\"\n",
        "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "print (model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='blue'>2.2.2 Model Training</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHHcxwpjfSm6",
        "outputId": "88b3de61-0b84-47f1-9d1e-a9030a4b0ac6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Source path =  G:\\My Drive\\Personal\\upGrad\\MS\\Recurrent Neural Networks\\Project_data\\train ; batch size = 32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ASyed\\AppData\\Local\\Temp\\ipykernel_46732\\842919994.py:26: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.4074 - categorical_accuracy: 0.5521Source path =  G:\\My Drive\\Personal\\upGrad\\MS\\Recurrent Neural Networks\\Project_data\\val ; batch size = 32\n",
            "\n",
            "Epoch 1: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00001-1.40741-0.55208-31.82745-0.27344.h5\n",
            "21/21 [==============================] - 158s 8s/step - loss: 1.4074 - categorical_accuracy: 0.5521 - val_loss: 31.8275 - val_categorical_accuracy: 0.2734 - lr: 0.0010\n",
            "Epoch 2/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.5045 - categorical_accuracy: 0.8259\n",
            "Epoch 2: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00002-0.50449-0.82589-22.73162-0.36719.h5\n",
            "21/21 [==============================] - 160s 8s/step - loss: 0.5045 - categorical_accuracy: 0.8259 - val_loss: 22.7316 - val_categorical_accuracy: 0.3672 - lr: 0.0010\n",
            "Epoch 3/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.1814 - categorical_accuracy: 0.9390\n",
            "Epoch 3: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00003-0.18143-0.93899-14.05996-0.41406.h5\n",
            "21/21 [==============================] - 136s 7s/step - loss: 0.1814 - categorical_accuracy: 0.9390 - val_loss: 14.0600 - val_categorical_accuracy: 0.4141 - lr: 0.0010\n",
            "Epoch 4/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0534 - categorical_accuracy: 0.9866\n",
            "Epoch 4: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00004-0.05341-0.98661-9.70952-0.47656.h5\n",
            "21/21 [==============================] - 154s 8s/step - loss: 0.0534 - categorical_accuracy: 0.9866 - val_loss: 9.7095 - val_categorical_accuracy: 0.4766 - lr: 0.0010\n",
            "Epoch 5/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0318 - categorical_accuracy: 0.9851\n",
            "Epoch 5: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00005-0.03178-0.98512-9.80005-0.43750.h5\n",
            "21/21 [==============================] - 149s 7s/step - loss: 0.0318 - categorical_accuracy: 0.9851 - val_loss: 9.8000 - val_categorical_accuracy: 0.4375 - lr: 0.0010\n",
            "Epoch 6/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0132 - categorical_accuracy: 0.9970\n",
            "Epoch 6: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00006-0.01317-0.99702-2.93209-0.57812.h5\n",
            "21/21 [==============================] - 161s 8s/step - loss: 0.0132 - categorical_accuracy: 0.9970 - val_loss: 2.9321 - val_categorical_accuracy: 0.5781 - lr: 0.0010\n",
            "Epoch 7/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0115 - categorical_accuracy: 0.9970\n",
            "Epoch 7: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00007-0.01153-0.99702-2.86880-0.50000.h5\n",
            "21/21 [==============================] - 157s 8s/step - loss: 0.0115 - categorical_accuracy: 0.9970 - val_loss: 2.8688 - val_categorical_accuracy: 0.5000 - lr: 0.0010\n",
            "Epoch 8/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0037 - categorical_accuracy: 1.0000\n",
            "Epoch 8: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00008-0.00368-1.00000-1.53002-0.67188.h5\n",
            "21/21 [==============================] - 156s 8s/step - loss: 0.0037 - categorical_accuracy: 1.0000 - val_loss: 1.5300 - val_categorical_accuracy: 0.6719 - lr: 0.0010\n",
            "Epoch 9/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0014 - categorical_accuracy: 1.0000\n",
            "Epoch 9: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00009-0.00137-1.00000-0.98312-0.71875.h5\n",
            "21/21 [==============================] - 160s 8s/step - loss: 0.0014 - categorical_accuracy: 1.0000 - val_loss: 0.9831 - val_categorical_accuracy: 0.7188 - lr: 0.0010\n",
            "Epoch 10/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 9.2551e-04 - categorical_accuracy: 1.0000\n",
            "Epoch 10: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00010-0.00093-1.00000-0.82821-0.74219.h5\n",
            "21/21 [==============================] - 158s 8s/step - loss: 9.2551e-04 - categorical_accuracy: 1.0000 - val_loss: 0.8282 - val_categorical_accuracy: 0.7422 - lr: 0.0010\n",
            "Epoch 11/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 7.2535e-04 - categorical_accuracy: 1.0000\n",
            "Epoch 11: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00011-0.00073-1.00000-0.76834-0.79688.h5\n",
            "21/21 [==============================] - 159s 8s/step - loss: 7.2535e-04 - categorical_accuracy: 1.0000 - val_loss: 0.7683 - val_categorical_accuracy: 0.7969 - lr: 0.0010\n",
            "Epoch 12/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 6.4044e-04 - categorical_accuracy: 1.0000\n",
            "Epoch 12: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00012-0.00064-1.00000-0.58292-0.81250.h5\n",
            "21/21 [==============================] - 169s 8s/step - loss: 6.4044e-04 - categorical_accuracy: 1.0000 - val_loss: 0.5829 - val_categorical_accuracy: 0.8125 - lr: 0.0010\n",
            "Epoch 13/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 5.2698e-04 - categorical_accuracy: 1.0000\n",
            "Epoch 13: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00013-0.00053-1.00000-0.53228-0.85938.h5\n",
            "21/21 [==============================] - 158s 8s/step - loss: 5.2698e-04 - categorical_accuracy: 1.0000 - val_loss: 0.5323 - val_categorical_accuracy: 0.8594 - lr: 0.0010\n",
            "Epoch 14/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 4.6037e-04 - categorical_accuracy: 1.0000\n",
            "Epoch 14: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00014-0.00046-1.00000-0.55291-0.85938.h5\n",
            "21/21 [==============================] - 160s 8s/step - loss: 4.6037e-04 - categorical_accuracy: 1.0000 - val_loss: 0.5529 - val_categorical_accuracy: 0.8594 - lr: 0.0010\n",
            "Epoch 15/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 3.7158e-04 - categorical_accuracy: 1.0000\n",
            "Epoch 15: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00015-0.00037-1.00000-0.42249-0.89844.h5\n",
            "21/21 [==============================] - 157s 8s/step - loss: 3.7158e-04 - categorical_accuracy: 1.0000 - val_loss: 0.4225 - val_categorical_accuracy: 0.8984 - lr: 0.0010\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x25a23342590>"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_generator = generator(train_path, train_doc, batch_size, img_idx, image_height, image_width)\n",
        "val_generator = generator(val_path, val_doc, batch_size, img_idx, image_height, image_width)\n",
        "\n",
        "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
        "\n",
        "if not os.path.exists(model_name):\n",
        "    os.mkdir(model_name)\n",
        "\n",
        "filepath = model_path_prefix + model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
        "\n",
        "LR = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=5, min_lr=0.001) # write the REducelronplateau code here\n",
        "callbacks_list = [checkpoint, LR]\n",
        "\n",
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1\n",
        "\n",
        "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
        "                    callbacks=callbacks_list, validation_data=val_generator,\n",
        "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='blue'>2.2.3 Accuracy & Remarks</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<b>Accuracy</b>:\n",
        "- Training: 1.0\n",
        "- Validation: 0.89\n",
        "\n",
        "<b>Remarks:</b>\n",
        "- Increased batch size + Image size, might have degraded performance\n",
        "- Validation Accuracy has increased\n",
        "- Training accuracy at 100% and High difference between Training vs Validation, shows model to be overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='blue'>2.2.4 Next Actions</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Introduced Dropout with 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font color='blue'>2.3 Model 3 - </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hS2rAehy2NJO"
      },
      "source": [
        "#### <font color='blue'>2.3.1 Design, Compile & Summary</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv3d_9 (Conv3D)           (None, 10, 100, 100, 16   1312      \n",
            "                             )                                   \n",
            "                                                                 \n",
            " max_pooling3d_9 (MaxPoolin  (None, 5, 50, 50, 16)     0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 5, 50, 50, 16)     0         \n",
            "                                                                 \n",
            " batch_normalization_9 (Bat  (None, 5, 50, 50, 16)     64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv3d_10 (Conv3D)          (None, 5, 50, 50, 32)     13856     \n",
            "                                                                 \n",
            " max_pooling3d_10 (MaxPooli  (None, 2, 25, 25, 32)     0         \n",
            " ng3D)                                                           \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 2, 25, 25, 32)     0         \n",
            "                                                                 \n",
            " batch_normalization_10 (Ba  (None, 2, 25, 25, 32)     128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv3d_11 (Conv3D)          (None, 2, 25, 25, 64)     55360     \n",
            "                                                                 \n",
            " max_pooling3d_11 (MaxPooli  (None, 1, 12, 12, 64)     0         \n",
            " ng3D)                                                           \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 1, 12, 12, 64)     0         \n",
            "                                                                 \n",
            " batch_normalization_11 (Ba  (None, 1, 12, 12, 64)     256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 9216)              0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 128)               1179776   \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 5)                 645       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1251397 (4.77 MB)\n",
            "Trainable params: 1251173 (4.77 MB)\n",
            "Non-trainable params: 224 (896.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "img_idx = list(range(1,30,3))\n",
        "frames = len(img_idx)\n",
        "image_height = 100\n",
        "image_width = 100\n",
        "batch_size = 32\n",
        "num_classes = 5\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv3D(16, (3,3,3), padding='same', input_shape=(frames, image_height, image_width, 3), activation='relu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv3D(32, (3,3,3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv3D(64, (3,3,3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "optimiser = \"adam\"\n",
        "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "print (model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='blue'>2.3.2 Model Training</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmR_XI-9nnaH",
        "outputId": "508807b0-b9e8-40fb-d59b-b4f9ad0f02a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Source path =  G:\\My Drive\\Personal\\upGrad\\MS\\Recurrent Neural Networks\\Project_data\\train ; batch size = 32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ASyed\\AppData\\Local\\Temp\\ipykernel_46732\\842919994.py:26: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 2.0307 - categorical_accuracy: 0.3423Source path =  G:\\My Drive\\Personal\\upGrad\\MS\\Recurrent Neural Networks\\Project_data\\val ; batch size = 32\n",
            "\n",
            "Epoch 1: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00001-2.03073-0.34226-8.60648-0.29688.h5\n",
            "21/21 [==============================] - 193s 9s/step - loss: 2.0307 - categorical_accuracy: 0.3423 - val_loss: 8.6065 - val_categorical_accuracy: 0.2969 - lr: 0.0010\n",
            "Epoch 2/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.4042 - categorical_accuracy: 0.4554\n",
            "Epoch 2: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00002-1.40424-0.45536-4.19344-0.27344.h5\n",
            "21/21 [==============================] - 186s 9s/step - loss: 1.4042 - categorical_accuracy: 0.4554 - val_loss: 4.1934 - val_categorical_accuracy: 0.2734 - lr: 0.0010\n",
            "Epoch 3/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.1220 - categorical_accuracy: 0.5580\n",
            "Epoch 3: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00003-1.12200-0.55804-2.34492-0.33594.h5\n",
            "21/21 [==============================] - 177s 9s/step - loss: 1.1220 - categorical_accuracy: 0.5580 - val_loss: 2.3449 - val_categorical_accuracy: 0.3359 - lr: 0.0010\n",
            "Epoch 4/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.9201 - categorical_accuracy: 0.6518\n",
            "Epoch 4: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00004-0.92009-0.65179-0.94233-0.67188.h5\n",
            "21/21 [==============================] - 157s 8s/step - loss: 0.9201 - categorical_accuracy: 0.6518 - val_loss: 0.9423 - val_categorical_accuracy: 0.6719 - lr: 0.0010\n",
            "Epoch 5/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.8896 - categorical_accuracy: 0.6503\n",
            "Epoch 5: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00005-0.88956-0.65030-0.73387-0.77344.h5\n",
            "21/21 [==============================] - 115s 6s/step - loss: 0.8896 - categorical_accuracy: 0.6503 - val_loss: 0.7339 - val_categorical_accuracy: 0.7734 - lr: 0.0010\n",
            "Epoch 6/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.7169 - categorical_accuracy: 0.7083\n",
            "Epoch 6: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00006-0.71692-0.70833-0.85306-0.67188.h5\n",
            "21/21 [==============================] - 140s 7s/step - loss: 0.7169 - categorical_accuracy: 0.7083 - val_loss: 0.8531 - val_categorical_accuracy: 0.6719 - lr: 0.0010\n",
            "Epoch 7/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.6966 - categorical_accuracy: 0.7307\n",
            "Epoch 7: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00007-0.69655-0.73065-1.03244-0.65625.h5\n",
            "21/21 [==============================] - 133s 7s/step - loss: 0.6966 - categorical_accuracy: 0.7307 - val_loss: 1.0324 - val_categorical_accuracy: 0.6562 - lr: 0.0010\n",
            "Epoch 8/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.5491 - categorical_accuracy: 0.7574\n",
            "Epoch 8: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00008-0.54915-0.75744-1.50521-0.52344.h5\n",
            "21/21 [==============================] - 135s 7s/step - loss: 0.5491 - categorical_accuracy: 0.7574 - val_loss: 1.5052 - val_categorical_accuracy: 0.5234 - lr: 0.0010\n",
            "Epoch 9/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.5340 - categorical_accuracy: 0.7842\n",
            "Epoch 9: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00009-0.53398-0.78423-1.62996-0.55469.h5\n",
            "21/21 [==============================] - 131s 7s/step - loss: 0.5340 - categorical_accuracy: 0.7842 - val_loss: 1.6300 - val_categorical_accuracy: 0.5547 - lr: 0.0010\n",
            "Epoch 10/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.4039 - categorical_accuracy: 0.8467\n",
            "Epoch 10: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00010-0.40394-0.84673-2.35831-0.45312.h5\n",
            "21/21 [==============================] - 123s 6s/step - loss: 0.4039 - categorical_accuracy: 0.8467 - val_loss: 2.3583 - val_categorical_accuracy: 0.4531 - lr: 0.0010\n",
            "Epoch 11/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.4152 - categorical_accuracy: 0.8452\n",
            "Epoch 11: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00011-0.41519-0.84524-1.83719-0.57031.h5\n",
            "21/21 [==============================] - 121s 6s/step - loss: 0.4152 - categorical_accuracy: 0.8452 - val_loss: 1.8372 - val_categorical_accuracy: 0.5703 - lr: 0.0010\n",
            "Epoch 12/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.3710 - categorical_accuracy: 0.8676\n",
            "Epoch 12: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00012-0.37101-0.86756-2.30844-0.50781.h5\n",
            "21/21 [==============================] - 125s 6s/step - loss: 0.3710 - categorical_accuracy: 0.8676 - val_loss: 2.3084 - val_categorical_accuracy: 0.5078 - lr: 0.0010\n",
            "Epoch 13/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.3533 - categorical_accuracy: 0.8720\n",
            "Epoch 13: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00013-0.35331-0.87202-2.59030-0.46094.h5\n",
            "21/21 [==============================] - 137s 7s/step - loss: 0.3533 - categorical_accuracy: 0.8720 - val_loss: 2.5903 - val_categorical_accuracy: 0.4609 - lr: 0.0010\n",
            "Epoch 14/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.3023 - categorical_accuracy: 0.8839\n",
            "Epoch 14: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00014-0.30230-0.88393-2.21334-0.53906.h5\n",
            "21/21 [==============================] - 134s 7s/step - loss: 0.3023 - categorical_accuracy: 0.8839 - val_loss: 2.2133 - val_categorical_accuracy: 0.5391 - lr: 0.0010\n",
            "Epoch 15/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.3401 - categorical_accuracy: 0.8705\n",
            "Epoch 15: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00015-0.34010-0.87054-1.72982-0.55469.h5\n",
            "21/21 [==============================] - 137s 7s/step - loss: 0.3401 - categorical_accuracy: 0.8705 - val_loss: 1.7298 - val_categorical_accuracy: 0.5547 - lr: 0.0010\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x25a27e2e1d0>"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_generator = generator(train_path, train_doc, batch_size, img_idx, image_height, image_width)\n",
        "val_generator = generator(val_path, val_doc, batch_size, img_idx, image_height, image_width)\n",
        "\n",
        "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
        "\n",
        "if not os.path.exists(model_name):\n",
        "    os.mkdir(model_name)\n",
        "\n",
        "filepath = model_path_prefix + model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
        "\n",
        "LR = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=5, min_lr=0.001) # write the REducelronplateau code here\n",
        "callbacks_list = [checkpoint, LR]\n",
        "\n",
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1\n",
        "\n",
        "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
        "                    callbacks=callbacks_list, validation_data=val_generator,\n",
        "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='blue'>2.3.3 Accuracy & Remarks</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<b>Accuracy</b>:\n",
        "- Training: 0.65\n",
        "- Validation: 0.77\n",
        "\n",
        "<b>Remarks:</b>\n",
        "- Suffered in both Training and Validation Accuracy\n",
        "- Training accuracy is more than Validation, issue in the learning itself"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='blue'>2.3.4 Next Actions</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lakRzPt0NA8l"
      },
      "source": [
        "Change architecture starting with 32 layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font color='blue'>2.4 Model 4 - </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='blue'>2.4.1 Design, Compile & Summary</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv3d_15 (Conv3D)          (None, 10, 100, 100, 32   2624      \n",
            "                             )                                   \n",
            "                                                                 \n",
            " max_pooling3d_15 (MaxPooli  (None, 5, 50, 50, 32)     0         \n",
            " ng3D)                                                           \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 5, 50, 50, 32)     0         \n",
            "                                                                 \n",
            " batch_normalization_15 (Ba  (None, 5, 50, 50, 32)     128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv3d_16 (Conv3D)          (None, 5, 50, 50, 64)     55360     \n",
            "                                                                 \n",
            " max_pooling3d_16 (MaxPooli  (None, 2, 25, 25, 64)     0         \n",
            " ng3D)                                                           \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 2, 25, 25, 64)     0         \n",
            "                                                                 \n",
            " batch_normalization_16 (Ba  (None, 2, 25, 25, 64)     256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv3d_17 (Conv3D)          (None, 2, 25, 25, 128)    221312    \n",
            "                                                                 \n",
            " max_pooling3d_17 (MaxPooli  (None, 1, 12, 12, 128)    0         \n",
            " ng3D)                                                           \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 1, 12, 12, 128)    0         \n",
            "                                                                 \n",
            " batch_normalization_17 (Ba  (None, 1, 12, 12, 128)    512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 18432)             0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 256)               4718848   \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 5)                 1285      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5000325 (19.07 MB)\n",
            "Trainable params: 4999877 (19.07 MB)\n",
            "Non-trainable params: 448 (1.75 KB)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "img_idx = list(range(1,30,3))\n",
        "frames = len(img_idx)\n",
        "image_height = 100\n",
        "image_width = 100\n",
        "batch_size = 64\n",
        "num_classes = 5\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv3D(32, (3,3,3), padding='same', input_shape=(frames, image_height, image_width, 3), activation='relu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv3D(64, (3,3,3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv3D(128, (3,3,3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "optimiser = \"adam\"\n",
        "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "print (model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='blue'>2.4.2 Model Training</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJ-gaR6o69-8",
        "outputId": "c8edc076-ebb3-40fa-9023-188bf59e85f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Source path =  G:\\My Drive\\Personal\\upGrad\\MS\\Recurrent Neural Networks\\Project_data\\train ; batch size = 64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ASyed\\AppData\\Local\\Temp\\ipykernel_46732\\3720141927.py:26: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 2.1899 - categorical_accuracy: 0.3466 Source path =  G:\\My Drive\\Personal\\upGrad\\MS\\Recurrent Neural Networks\\Project_data\\val ; batch size = 64\n",
            "\n",
            "Epoch 1: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00001-2.18986-0.34659-40.54904-0.26562.h5\n",
            "11/11 [==============================] - 301s 30s/step - loss: 2.1899 - categorical_accuracy: 0.3466 - val_loss: 40.5490 - val_categorical_accuracy: 0.2656 - lr: 0.0010\n",
            "Epoch 2/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 1.4236 - categorical_accuracy: 0.4986 \n",
            "Epoch 2: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00002-1.42362-0.49858-40.92770-0.23438.h5\n",
            "11/11 [==============================] - 336s 33s/step - loss: 1.4236 - categorical_accuracy: 0.4986 - val_loss: 40.9277 - val_categorical_accuracy: 0.2344 - lr: 0.0010\n",
            "Epoch 3/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 1.0030 - categorical_accuracy: 0.6207 \n",
            "Epoch 3: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00003-1.00302-0.62074-27.05127-0.23438.h5\n",
            "11/11 [==============================] - 301s 30s/step - loss: 1.0030 - categorical_accuracy: 0.6207 - val_loss: 27.0513 - val_categorical_accuracy: 0.2344 - lr: 0.0010\n",
            "Epoch 4/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.8836 - categorical_accuracy: 0.6719 \n",
            "Epoch 4: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00004-0.88359-0.67188-25.91345-0.21875.h5\n",
            "11/11 [==============================] - 318s 31s/step - loss: 0.8836 - categorical_accuracy: 0.6719 - val_loss: 25.9134 - val_categorical_accuracy: 0.2188 - lr: 0.0010\n",
            "Epoch 5/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.7434 - categorical_accuracy: 0.7003 \n",
            "Epoch 5: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00005-0.74337-0.70028-15.99927-0.31250.h5\n",
            "11/11 [==============================] - 367s 36s/step - loss: 0.7434 - categorical_accuracy: 0.7003 - val_loss: 15.9993 - val_categorical_accuracy: 0.3125 - lr: 0.0010\n",
            "Epoch 6/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.6194 - categorical_accuracy: 0.7585 \n",
            "Epoch 6: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00006-0.61941-0.75852-5.38813-0.34375.h5\n",
            "11/11 [==============================] - 520s 51s/step - loss: 0.6194 - categorical_accuracy: 0.7585 - val_loss: 5.3881 - val_categorical_accuracy: 0.3438 - lr: 0.0010\n",
            "Epoch 7/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.5345 - categorical_accuracy: 0.8011 \n",
            "Epoch 7: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00007-0.53449-0.80114-3.67095-0.45312.h5\n",
            "11/11 [==============================] - 729s 72s/step - loss: 0.5345 - categorical_accuracy: 0.8011 - val_loss: 3.6709 - val_categorical_accuracy: 0.4531 - lr: 0.0010\n",
            "Epoch 8/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.4653 - categorical_accuracy: 0.8253 \n",
            "Epoch 8: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00008-0.46529-0.82528-2.50583-0.50000.h5\n",
            "11/11 [==============================] - 598s 59s/step - loss: 0.4653 - categorical_accuracy: 0.8253 - val_loss: 2.5058 - val_categorical_accuracy: 0.5000 - lr: 0.0010\n",
            "Epoch 9/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.4556 - categorical_accuracy: 0.8253 \n",
            "Epoch 9: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00009-0.45561-0.82528-2.86569-0.48438.h5\n",
            "11/11 [==============================] - 435s 43s/step - loss: 0.4556 - categorical_accuracy: 0.8253 - val_loss: 2.8657 - val_categorical_accuracy: 0.4844 - lr: 0.0010\n",
            "Epoch 10/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.3615 - categorical_accuracy: 0.8651 \n",
            "Epoch 10: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00010-0.36152-0.86506-0.84944-0.75781.h5\n",
            "11/11 [==============================] - 605s 60s/step - loss: 0.3615 - categorical_accuracy: 0.8651 - val_loss: 0.8494 - val_categorical_accuracy: 0.7578 - lr: 0.0010\n",
            "Epoch 11/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2467 - categorical_accuracy: 0.9091 \n",
            "Epoch 11: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00011-0.24671-0.90909-0.75231-0.81250.h5\n",
            "11/11 [==============================] - 611s 60s/step - loss: 0.2467 - categorical_accuracy: 0.9091 - val_loss: 0.7523 - val_categorical_accuracy: 0.8125 - lr: 0.0010\n",
            "Epoch 12/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2725 - categorical_accuracy: 0.8935 \n",
            "Epoch 12: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00012-0.27254-0.89347-0.77396-0.82031.h5\n",
            "11/11 [==============================] - 323s 32s/step - loss: 0.2725 - categorical_accuracy: 0.8935 - val_loss: 0.7740 - val_categorical_accuracy: 0.8203 - lr: 0.0010\n",
            "Epoch 13/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1965 - categorical_accuracy: 0.9233 \n",
            "Epoch 13: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00013-0.19651-0.92330-1.07000-0.76562.h5\n",
            "11/11 [==============================] - 309s 30s/step - loss: 0.1965 - categorical_accuracy: 0.9233 - val_loss: 1.0700 - val_categorical_accuracy: 0.7656 - lr: 0.0010\n",
            "Epoch 14/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1757 - categorical_accuracy: 0.9332 \n",
            "Epoch 14: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00014-0.17568-0.93324-1.18008-0.70312.h5\n",
            "11/11 [==============================] - 302s 30s/step - loss: 0.1757 - categorical_accuracy: 0.9332 - val_loss: 1.1801 - val_categorical_accuracy: 0.7031 - lr: 0.0010\n",
            "Epoch 15/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1721 - categorical_accuracy: 0.9418 \n",
            "Epoch 15: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00015-0.17209-0.94176-1.18958-0.70312.h5\n",
            "11/11 [==============================] - 301s 30s/step - loss: 0.1721 - categorical_accuracy: 0.9418 - val_loss: 1.1896 - val_categorical_accuracy: 0.7031 - lr: 5.0000e-04\n"
          ]
        }
      ],
      "source": [
        "train_generator = generator(train_path, train_doc, batch_size, img_idx, image_height, image_width)\n",
        "val_generator = generator(val_path, val_doc, batch_size, img_idx, image_height, image_width)\n",
        "\n",
        "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
        "\n",
        "if not os.path.exists(model_name):\n",
        "    os.mkdir(model_name)\n",
        "\n",
        "filepath = model_path_prefix + model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
        "\n",
        "LR = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-5) # write the REducelronplateau code here\n",
        "callbacks_list = [checkpoint, LR]\n",
        "\n",
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1\n",
        "\n",
        "history = model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
        "                    callbacks=callbacks_list, validation_data=val_generator,\n",
        "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='blue'>2.4.3 Accuracy & Remarks</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<b>Accuracy</b>:\n",
        "- Training: 0.89\n",
        "- Validation: 0.82\n",
        "\n",
        "<b>Remarks:</b>\n",
        "- Training and Validation Accuracy increased\n",
        "- Validation accuracy is not in acceptable range\n",
        "- High number of trainable params increasing the model size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='blue'>2.4.4 Next Actions</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xE-H3yuTg8hy"
      },
      "source": [
        "- Increase batch size to 64\n",
        "- Crop image to 50 x 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font color='blue'>2.5 Model 5 - </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='blue'>2.5.1 Design, Compile & Summary</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv3d_18 (Conv3D)          (None, 10, 50, 50, 32)    2624      \n",
            "                                                                 \n",
            " max_pooling3d_18 (MaxPooli  (None, 5, 25, 25, 32)     0         \n",
            " ng3D)                                                           \n",
            "                                                                 \n",
            " dropout_12 (Dropout)        (None, 5, 25, 25, 32)     0         \n",
            "                                                                 \n",
            " batch_normalization_18 (Ba  (None, 5, 25, 25, 32)     128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv3d_19 (Conv3D)          (None, 5, 25, 25, 64)     55360     \n",
            "                                                                 \n",
            " max_pooling3d_19 (MaxPooli  (None, 2, 12, 12, 64)     0         \n",
            " ng3D)                                                           \n",
            "                                                                 \n",
            " dropout_13 (Dropout)        (None, 2, 12, 12, 64)     0         \n",
            "                                                                 \n",
            " batch_normalization_19 (Ba  (None, 2, 12, 12, 64)     256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv3d_20 (Conv3D)          (None, 2, 12, 12, 128)    221312    \n",
            "                                                                 \n",
            " max_pooling3d_20 (MaxPooli  (None, 1, 6, 6, 128)      0         \n",
            " ng3D)                                                           \n",
            "                                                                 \n",
            " dropout_14 (Dropout)        (None, 1, 6, 6, 128)      0         \n",
            "                                                                 \n",
            " batch_normalization_20 (Ba  (None, 1, 6, 6, 128)      512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 4608)              0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 256)               1179904   \n",
            "                                                                 \n",
            " dropout_15 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 5)                 1285      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1461381 (5.57 MB)\n",
            "Trainable params: 1460933 (5.57 MB)\n",
            "Non-trainable params: 448 (1.75 KB)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "img_idx = list(range(1,30,3))\n",
        "frames = len(img_idx)\n",
        "image_height = 50\n",
        "image_width = 50\n",
        "batch_size = 64\n",
        "num_classes = 5\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv3D(32, (3,3,3), padding='same', input_shape=(frames, image_height, image_width, 3), activation='relu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv3D(64, (3,3,3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv3D(128, (3,3,3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "optimiser = \"adam\"\n",
        "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "print (model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='blue'>2.5.2 Model Training</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yjNjRE0Tg5hx",
        "outputId": "073e367e-a352-4326-b5fb-3a3864be1341"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Source path =  G:\\My Drive\\Personal\\upGrad\\MS\\Recurrent Neural Networks\\Project_data\\train ; batch size = 64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ASyed\\AppData\\Local\\Temp\\ipykernel_46732\\3720141927.py:26: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 2.2656 - categorical_accuracy: 0.3139 Source path =  G:\\My Drive\\Personal\\upGrad\\MS\\Recurrent Neural Networks\\Project_data\\val ; batch size = 64\n",
            "\n",
            "Epoch 1: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00001-2.26564-0.31392-20.95386-0.20312.h5\n",
            "11/11 [==============================] - 298s 29s/step - loss: 2.2656 - categorical_accuracy: 0.3139 - val_loss: 20.9539 - val_categorical_accuracy: 0.2031 - lr: 0.0010\n",
            "Epoch 2/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 1.6287 - categorical_accuracy: 0.4119 \n",
            "Epoch 2: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00002-1.62866-0.41193-16.19118-0.24219.h5\n",
            "11/11 [==============================] - 251s 25s/step - loss: 1.6287 - categorical_accuracy: 0.4119 - val_loss: 16.1912 - val_categorical_accuracy: 0.2422 - lr: 0.0010\n",
            "Epoch 3/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 1.4189 - categorical_accuracy: 0.5057 \n",
            "Epoch 3: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00003-1.41891-0.50568-13.20505-0.17188.h5\n",
            "11/11 [==============================] - 193s 19s/step - loss: 1.4189 - categorical_accuracy: 0.5057 - val_loss: 13.2051 - val_categorical_accuracy: 0.1719 - lr: 0.0010\n",
            "Epoch 4/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 1.2566 - categorical_accuracy: 0.5241 \n",
            "Epoch 4: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00004-1.25662-0.52415-9.33715-0.22656.h5\n",
            "11/11 [==============================] - 202s 20s/step - loss: 1.2566 - categorical_accuracy: 0.5241 - val_loss: 9.3372 - val_categorical_accuracy: 0.2266 - lr: 0.0010\n",
            "Epoch 5/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 1.1263 - categorical_accuracy: 0.5526 \n",
            "Epoch 5: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00005-1.12633-0.55256-10.12846-0.20312.h5\n",
            "11/11 [==============================] - 234s 23s/step - loss: 1.1263 - categorical_accuracy: 0.5526 - val_loss: 10.1285 - val_categorical_accuracy: 0.2031 - lr: 0.0010\n",
            "Epoch 6/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.9559 - categorical_accuracy: 0.6236 \n",
            "Epoch 6: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00006-0.95590-0.62358-5.23939-0.21875.h5\n",
            "11/11 [==============================] - 238s 24s/step - loss: 0.9559 - categorical_accuracy: 0.6236 - val_loss: 5.2394 - val_categorical_accuracy: 0.2188 - lr: 0.0010\n",
            "Epoch 7/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.8897 - categorical_accuracy: 0.6250 \n",
            "Epoch 7: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00007-0.88968-0.62500-3.10506-0.28125.h5\n",
            "11/11 [==============================] - 252s 25s/step - loss: 0.8897 - categorical_accuracy: 0.6250 - val_loss: 3.1051 - val_categorical_accuracy: 0.2812 - lr: 0.0010\n",
            "Epoch 8/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.7663 - categorical_accuracy: 0.6761 \n",
            "Epoch 8: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00008-0.76626-0.67614-2.16280-0.41406.h5\n",
            "11/11 [==============================] - 229s 23s/step - loss: 0.7663 - categorical_accuracy: 0.6761 - val_loss: 2.1628 - val_categorical_accuracy: 0.4141 - lr: 0.0010\n",
            "Epoch 9/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.7442 - categorical_accuracy: 0.7088 \n",
            "Epoch 9: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00009-0.74422-0.70881-1.17286-0.60938.h5\n",
            "11/11 [==============================] - 241s 24s/step - loss: 0.7442 - categorical_accuracy: 0.7088 - val_loss: 1.1729 - val_categorical_accuracy: 0.6094 - lr: 0.0010\n",
            "Epoch 10/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.6725 - categorical_accuracy: 0.7514 \n",
            "Epoch 10: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00010-0.67251-0.75142-0.80832-0.71094.h5\n",
            "11/11 [==============================] - 225s 22s/step - loss: 0.6725 - categorical_accuracy: 0.7514 - val_loss: 0.8083 - val_categorical_accuracy: 0.7109 - lr: 0.0010\n",
            "Epoch 11/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.5936 - categorical_accuracy: 0.7713 \n",
            "Epoch 11: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00011-0.59356-0.77131-0.76107-0.79688.h5\n",
            "11/11 [==============================] - 208s 21s/step - loss: 0.5936 - categorical_accuracy: 0.7713 - val_loss: 0.7611 - val_categorical_accuracy: 0.7969 - lr: 0.0010\n",
            "Epoch 12/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.5393 - categorical_accuracy: 0.7969 \n",
            "Epoch 12: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00012-0.53926-0.79688-0.60273-0.79688.h5\n",
            "11/11 [==============================] - 236s 23s/step - loss: 0.5393 - categorical_accuracy: 0.7969 - val_loss: 0.6027 - val_categorical_accuracy: 0.7969 - lr: 0.0010\n",
            "Epoch 13/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.5840 - categorical_accuracy: 0.7656 \n",
            "Epoch 13: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00013-0.58400-0.76562-0.71583-0.75000.h5\n",
            "11/11 [==============================] - 241s 24s/step - loss: 0.5840 - categorical_accuracy: 0.7656 - val_loss: 0.7158 - val_categorical_accuracy: 0.7500 - lr: 0.0010\n",
            "Epoch 14/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.5241 - categorical_accuracy: 0.8097 \n",
            "Epoch 14: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00014-0.52406-0.80966-0.83018-0.75781.h5\n",
            "11/11 [==============================] - 198s 20s/step - loss: 0.5241 - categorical_accuracy: 0.8097 - val_loss: 0.8302 - val_categorical_accuracy: 0.7578 - lr: 0.0010\n",
            "Epoch 15/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.4681 - categorical_accuracy: 0.8224 \n",
            "Epoch 15: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00015-0.46809-0.82244-0.74213-0.75000.h5\n",
            "11/11 [==============================] - 169s 17s/step - loss: 0.4681 - categorical_accuracy: 0.8224 - val_loss: 0.7421 - val_categorical_accuracy: 0.7500 - lr: 0.0010\n"
          ]
        }
      ],
      "source": [
        "train_generator = generator(train_path, train_doc, batch_size, img_idx, image_height, image_width)\n",
        "val_generator = generator(val_path, val_doc, batch_size, img_idx, image_height, image_width)\n",
        "\n",
        "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
        "\n",
        "if not os.path.exists(model_name):\n",
        "    os.mkdir(model_name)\n",
        "\n",
        "filepath = model_path_prefix + model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
        "\n",
        "LR = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-5) # write the REducelronplateau code here\n",
        "callbacks_list = [checkpoint, LR]\n",
        "\n",
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1\n",
        "\n",
        "history = model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
        "                    callbacks=callbacks_list, validation_data=val_generator,\n",
        "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='blue'>2.5.3 Accuracy & Remarks</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<b>Accuracy</b>:\n",
        "- Training: 0.79\n",
        "- Validation: 0.79\n",
        "\n",
        "<b>Remarks:</b>\n",
        "- Impacted performance a lot\n",
        "- Validation accuracy decreased\n",
        "- Both Training and Valdiation accuracies are in acceptable range and there isnt much difference either"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='blue'>2.5.4 Next Actions</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7GMAyyqknPR"
      },
      "source": [
        "- Reduce batch size to 32\n",
        "- Change architecture to start with 8 layers, to reduce size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font color='blue'>2.6 Model 6 - </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='blue'>2.6.1 Design, Compile & Summary</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv3d_21 (Conv3D)          (None, 10, 50, 50, 8)     656       \n",
            "                                                                 \n",
            " max_pooling3d_21 (MaxPooli  (None, 5, 25, 25, 8)      0         \n",
            " ng3D)                                                           \n",
            "                                                                 \n",
            " dropout_16 (Dropout)        (None, 5, 25, 25, 8)      0         \n",
            "                                                                 \n",
            " batch_normalization_21 (Ba  (None, 5, 25, 25, 8)      32        \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv3d_22 (Conv3D)          (None, 5, 25, 25, 16)     3472      \n",
            "                                                                 \n",
            " max_pooling3d_22 (MaxPooli  (None, 2, 12, 12, 16)     0         \n",
            " ng3D)                                                           \n",
            "                                                                 \n",
            " dropout_17 (Dropout)        (None, 2, 12, 12, 16)     0         \n",
            "                                                                 \n",
            " batch_normalization_22 (Ba  (None, 2, 12, 12, 16)     64        \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv3d_23 (Conv3D)          (None, 2, 12, 12, 32)     13856     \n",
            "                                                                 \n",
            " max_pooling3d_23 (MaxPooli  (None, 1, 6, 6, 32)       0         \n",
            " ng3D)                                                           \n",
            "                                                                 \n",
            " dropout_18 (Dropout)        (None, 1, 6, 6, 32)       0         \n",
            "                                                                 \n",
            " batch_normalization_23 (Ba  (None, 1, 6, 6, 32)       128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " flatten_7 (Flatten)         (None, 1152)              0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 64)                73792     \n",
            "                                                                 \n",
            " dropout_19 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 5)                 325       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 92325 (360.64 KB)\n",
            "Trainable params: 92213 (360.21 KB)\n",
            "Non-trainable params: 112 (448.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "img_idx = list(range(1,30,3))\n",
        "frames = len(img_idx)\n",
        "image_height = 50\n",
        "image_width = 50\n",
        "batch_size = 32\n",
        "num_classes = 5\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv3D(8, (3,3,3), padding='same', input_shape=(frames, image_height, image_width, 3), activation='relu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv3D(16, (3,3,3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv3D(32, (3,3,3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "optimiser = \"adam\"\n",
        "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "print (model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='blue'>2.6.2 Model Training</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-SuLH3OykglP",
        "outputId": "d841db2f-80cf-40c0-9dfb-c0b30afcc2fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Source path =  G:\\My Drive\\Personal\\upGrad\\MS\\Recurrent Neural Networks\\Project_data\\train ; batch size = 32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ASyed\\AppData\\Local\\Temp\\ipykernel_46732\\3720141927.py:26: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 2.2661 - categorical_accuracy: 0.2158Source path =  G:\\My Drive\\Personal\\upGrad\\MS\\Recurrent Neural Networks\\Project_data\\val ; batch size = 32\n",
            "\n",
            "Epoch 1: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00001-2.26614-0.21577-3.01016-0.22656.h5\n",
            "21/21 [==============================] - 254s 12s/step - loss: 2.2661 - categorical_accuracy: 0.2158 - val_loss: 3.0102 - val_categorical_accuracy: 0.2266 - lr: 0.0010\n",
            "Epoch 2/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.9355 - categorical_accuracy: 0.2455\n",
            "Epoch 2: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00002-1.93554-0.24554-1.87922-0.24219.h5\n",
            "21/21 [==============================] - 242s 12s/step - loss: 1.9355 - categorical_accuracy: 0.2455 - val_loss: 1.8792 - val_categorical_accuracy: 0.2422 - lr: 0.0010\n",
            "Epoch 3/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.6014 - categorical_accuracy: 0.3214\n",
            "Epoch 3: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00003-1.60138-0.32143-1.37295-0.35156.h5\n",
            "21/21 [==============================] - 226s 11s/step - loss: 1.6014 - categorical_accuracy: 0.3214 - val_loss: 1.3730 - val_categorical_accuracy: 0.3516 - lr: 0.0010\n",
            "Epoch 4/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.4673 - categorical_accuracy: 0.3869\n",
            "Epoch 4: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00004-1.46727-0.38690-1.20447-0.51562.h5\n",
            "21/21 [==============================] - 212s 11s/step - loss: 1.4673 - categorical_accuracy: 0.3869 - val_loss: 1.2045 - val_categorical_accuracy: 0.5156 - lr: 0.0010\n",
            "Epoch 5/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.4445 - categorical_accuracy: 0.4033\n",
            "Epoch 5: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00005-1.44452-0.40327-1.19393-0.46875.h5\n",
            "21/21 [==============================] - 210s 10s/step - loss: 1.4445 - categorical_accuracy: 0.4033 - val_loss: 1.1939 - val_categorical_accuracy: 0.4688 - lr: 0.0010\n",
            "Epoch 6/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.3131 - categorical_accuracy: 0.4479\n",
            "Epoch 6: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00006-1.31306-0.44792-1.33560-0.37500.h5\n",
            "21/21 [==============================] - 220s 11s/step - loss: 1.3131 - categorical_accuracy: 0.4479 - val_loss: 1.3356 - val_categorical_accuracy: 0.3750 - lr: 0.0010\n",
            "Epoch 7/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.2054 - categorical_accuracy: 0.4658\n",
            "Epoch 7: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00007-1.20545-0.46577-1.58139-0.36719.h5\n",
            "21/21 [==============================] - 205s 10s/step - loss: 1.2054 - categorical_accuracy: 0.4658 - val_loss: 1.5814 - val_categorical_accuracy: 0.3672 - lr: 0.0010\n",
            "Epoch 8/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.2003 - categorical_accuracy: 0.4702\n",
            "Epoch 8: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00008-1.20033-0.47024-1.81514-0.31250.h5\n",
            "21/21 [==============================] - 212s 11s/step - loss: 1.2003 - categorical_accuracy: 0.4702 - val_loss: 1.8151 - val_categorical_accuracy: 0.3125 - lr: 0.0010\n",
            "Epoch 9/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.1272 - categorical_accuracy: 0.5179\n",
            "Epoch 9: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00009-1.12723-0.51786-2.11647-0.29688.h5\n",
            "21/21 [==============================] - 222s 11s/step - loss: 1.1272 - categorical_accuracy: 0.5179 - val_loss: 2.1165 - val_categorical_accuracy: 0.2969 - lr: 5.0000e-04\n",
            "Epoch 10/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.1391 - categorical_accuracy: 0.5000\n",
            "Epoch 10: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00010-1.13906-0.50000-2.18634-0.27344.h5\n",
            "21/21 [==============================] - 211s 11s/step - loss: 1.1391 - categorical_accuracy: 0.5000 - val_loss: 2.1863 - val_categorical_accuracy: 0.2734 - lr: 5.0000e-04\n",
            "Epoch 11/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.1318 - categorical_accuracy: 0.5208\n",
            "Epoch 11: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00011-1.13176-0.52083-2.17216-0.29688.h5\n",
            "21/21 [==============================] - 206s 10s/step - loss: 1.1318 - categorical_accuracy: 0.5208 - val_loss: 2.1722 - val_categorical_accuracy: 0.2969 - lr: 5.0000e-04\n",
            "Epoch 12/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.1470 - categorical_accuracy: 0.5298\n",
            "Epoch 12: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00012-1.14697-0.52976-2.20980-0.30469.h5\n",
            "21/21 [==============================] - 225s 11s/step - loss: 1.1470 - categorical_accuracy: 0.5298 - val_loss: 2.2098 - val_categorical_accuracy: 0.3047 - lr: 2.5000e-04\n",
            "Epoch 13/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.0983 - categorical_accuracy: 0.5402\n",
            "Epoch 13: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00013-1.09830-0.54018-2.56883-0.25781.h5\n",
            "21/21 [==============================] - 206s 10s/step - loss: 1.0983 - categorical_accuracy: 0.5402 - val_loss: 2.5688 - val_categorical_accuracy: 0.2578 - lr: 2.5000e-04\n",
            "Epoch 14/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.0989 - categorical_accuracy: 0.5045\n",
            "Epoch 14: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00014-1.09888-0.50446-2.49511-0.25781.h5\n",
            "21/21 [==============================] - 207s 10s/step - loss: 1.0989 - categorical_accuracy: 0.5045 - val_loss: 2.4951 - val_categorical_accuracy: 0.2578 - lr: 2.5000e-04\n",
            "Epoch 15/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.0652 - categorical_accuracy: 0.5521\n",
            "Epoch 15: saving model to C:\\Temp\\RNN\\model_init_2024-03-0518_28_54.709195\\model-00015-1.06515-0.55208-2.52690-0.25000.h5\n",
            "21/21 [==============================] - 217s 11s/step - loss: 1.0652 - categorical_accuracy: 0.5521 - val_loss: 2.5269 - val_categorical_accuracy: 0.2500 - lr: 1.2500e-04\n"
          ]
        }
      ],
      "source": [
        "train_generator = generator(train_path, train_doc, batch_size, img_idx, image_height, image_width)\n",
        "val_generator = generator(val_path, val_doc, batch_size, img_idx, image_height, image_width)\n",
        "\n",
        "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
        "\n",
        "if not os.path.exists(model_name):\n",
        "    os.mkdir(model_name)\n",
        "\n",
        "filepath = model_path_prefix + model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
        "\n",
        "LR = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-5) # write the REducelronplateau code here\n",
        "callbacks_list = [checkpoint, LR]\n",
        "\n",
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1\n",
        "\n",
        "history = model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
        "                    callbacks=callbacks_list, validation_data=val_generator,\n",
        "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='blue'>2.6.3 Accuracy & Remarks</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<b>Accuracy</b>:\n",
        "- Training: 0.38\n",
        "- Validation: 0.51\n",
        "\n",
        "<b>Remarks:</b>\n",
        "- Training and Validation Accuracies are least of all models, not acceptable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='blue'>2.6.4 Next Actions</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we already have some good performing models, will choose one from those"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <font color='blue'>3. Conclusion & Final Selection</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Selected the best model from Model  4 with,\n",
        "- <b>Training Accuracy: </b>0.89\n",
        "- <b>Validation Accuracy: </b>0.82\n",
        "- <b>Batch Size: </b>32\n",
        "- <b>Frames: </b>10\n",
        "- <b>Image Height: </b>100\n",
        "- <b>Image Width: </b>100\n",
        "- <b>Drop out: </b>Yes, 0.5\n",
        "- <b>Batch Normalization: </b>Yes\n",
        "- <b>Layers: </b>32 > 64 > 128 >> 256 >> 5\n",
        "- <b>Model File Name: </b>model-00012-0.27254-0.89347-0.77396-0.82031.h5\n",
        "- <b>Model File Size: </b>57.2 MB\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
